---
title: "Income Per Capita Study"
author: "Raphael Nash & Luis Calleja"
date: "December 21, 2016"
output: html_document
---

```{r, warning=F, message=F,echo=F}
library(stringr)
library(reshape)
library(dplyr)
library(stringr)
library(knitr)
library(httr)
library(readr)
library(tidyr)
library(ggplot2)

save.image('~/Documents/CUNY/data_class/project-final/final_presentation.RData')
```

Because the data looked odd and counties like Kings and Manhattan came in at around $40K income per capita, we decided to make adjustments for the impoverished - knowing that they could skew the data.
```{r, warning=F}
bea_2010_length <- length(read_lines("https://raw.githubusercontent.com/RaphaelNash/CUNY-DATA-607-Final-Project/master/bea_income_per_capita/ipc_2010.csv"))

bea_2010 <- read.csv( "https://raw.githubusercontent.com/RaphaelNash/CUNY-DATA-607-Final-Project/master/bea_income_per_capita/ipc_2010.csv", nrows = (bea_2010_length-16), skip = 4, stringsAsFactors = FALSE)

colnames(bea_2010) <- c("fips", "GeoName", "income_per_capita_2010")

bea_2010 <- subset(bea_2010, select = c(1,3))

bea_2010$income_per_capita_2010 <- as.numeric(bea_2010$income_per_capita_2010)
```

Let's take a look at counties in NYC (Queens, Bronx, Manhattan, Brooklyn) and compare with counties from South Florida (Broward, Miami-Dade, Monroe, Collier):
```{r}
kable(bea_2010[bea_2010$fips %in% c(12086,12011,12087,12021),])

kable(bea_2010[bea_2010$fips %in% c(36047,36081,36061,36005),])
```

We looked into the data to determine how these numbers could show such a surprising picture for these two areas. 

```{r poverty_adj_income.R_1, echo=FALSE}
cnames<-c('ID','fips','cnty.name','apr.1.2010','base.2010','july.2010','2011','2012','2013','2014')
cls<-c(fips="character")
census.trim<-read.table('~/Documents/CUNY/data_class/project-final/census_data/census_2014_trimmed/PEP_2014_PEPANNRES_with_ann.csv',stringsAsFactors = F,sep=",",skip=2,header = F,col.names = cnames,colClasses = cls)
#using Raphael's original file - a mess
census<-readLines('~/Documents/CUNY/data_class/project-final/census2014.txt')
census.1<-unlist(str_split(census,'\n'))
```

```{r poverty_adj_income.R_2, echo=FALSE}
#total population by county by year: census 
cnames<-c('ID','fips','cnty.name','apr.1.2010','base.2010','july.2010','2011','2012','2013','2014')
cls<-c(fips="character")
census.trim<-read.table('~/Documents/CUNY/data_class/project-final/census_data/census_2014_trimmed/PEP_2014_PEPANNRES_with_ann.csv',stringsAsFactors = F,sep=",",skip=2,header = F,col.names = cnames,colClasses = cls)
#using Raphael's original file - a mess
census<-readLines('~/Documents/CUNY/data_class/project-final/census2014.txt')
census.1<-unlist(str_split(census,'\n'))
```

We consider census data (source: https://www.census.gov/did/www/saipe/downloads/estmod14/readme.txt) on poverty for each county. Disclaimer: we only consider estimates from 2014 (latest year available is 2015).
```{r poverty_adj_income.R_3}
cls=c(t.fips='character')
poverty<-read.csv('~/Documents/CUNY/data_class/project-final/treatedMedian2014.csv',stringsAsFactors = F,header=T,colClasses = cls)
```

Let's take a look at the size of population deemed 'poor' per county for each of these areas:
```{r poverty_adj_income.R_4}
#gross income of county by year
kable(poverty[poverty$t.fips %in% c(12086,12011,12087,12021),c(1,6)])

kable(poverty[poverty$t.fips %in% c(36047,36081,36061,36005),c(1,6)])
```

```{r poverty_adj_income.R_5,echo=FALSE}
cls=c(GeoFips='character')
personal.inc<-read.csv('~/Documents/CUNY/data_class/project-final/personal_inc_2015.csv',stringsAsFactors = F,skip=4,header=T,colClasses = cls)
```

We will apply an adjustment calculation in order to arrive at an adjusted income per capita. The calculation looks like this       

$total.personal.income/(census.population - total.poverty.population)$   

We remove the size of the poor population divide this into the total income generated by that county.
```{r poverty_adj_income.R_6}
#change all the names to prep for merge_all()
names(personal.inc)<-c("fips","GeoName","total.pers.inc")
names(poverty)[6]<-"fips"
n.l<-names(census.trim)
names(census.trim)<-sapply(n.l,paste0,".census")
names(census.trim)[2]<-c("fips")
#run the merge_all calc
df<-list(personal.inc,poverty,census.trim)
que<-Reduce(merge,df)

g.stack<-que %>%
  mutate(adj.per.capita=as.numeric(total.pers.inc)/(X2014.census-all.poverty), adj.per.capita=adj.per.capita*1000)
#names(g.stack)
```
Our reference point is the other sterilized summary statistics, the median.
```{r}
#South FLorida
kable(g.stack[g.stack$fips %in% c(12086,12011,12087,12021),c(1,6)])

#New York City
kable(g.stack[g.stack$fips %in% c(36047,36081,36061,36005),c(1,6)])
```

Percentiles on the size of impact from the adjustment reveal that the adjusted figures tended to be to the upside when compared with median.
```{r poverty_adj_income.R_7}
summary(g.stack$adj.per.capita-g.stack$med.inc)
```

With our adjustments and data in hand, we build an algorithm to cluster and classify counties as high income and other (or low). This is all done in R and was time consuming.   
**Disclaimer:** We classify counties using only 2014 data: both poverty and total personal income.   

```{r final_analysis.R}
load('~/Documents/CUNY/data_class/project-final/final_algo.RData')
#merge to original data table
full<-merge(g.stack,cluster.df,by='fips')
#compute y/y growth
test<-full %>%
  mutate('2011.g'=1-(X2011.census/base.2010.census), '2012.g'=1-(X2012.census/X2011.census), '2013.g'=1-(X2013.census/X2012.census), '2014.g'=1-(X2012.census/X2011.census))

test[test$label=="eligible","label"]<-"low"
#main portion of table that is interesting:
full.t<-test[,c(1,19:23)]
#recode
full.t[full.t$label=="eligible","label"]<-"low"
colnames(full.t)[3:6]<-c('g.2011','g.2012','g.2013','g.2014')
#long format
ma<-full.t %>%
  gather(year,growth.rate,g.2011:g.2014)
```

For the sake of argument, we run this same plot using only median income for 2014.
```{r}
ggplot(test,aes(x=label,y=med.inc)) +geom_boxplot() +ggtitle("US Census Median Income")

ggplot(test,aes(x=label,y=adj.per.capita)) +geom_boxplot() +ggtitle("Adjusted Income Per Capita Measure")
```

With our labels in hand we are ready to apply summary statistics and determine whether income per capita indeed has a positive relationships to growth.

Run some visualization... box plots classed on label
```{r}
ggplot(data=ma,aes(x=year,y=growth.rate, fill=label)) + geom_boxplot() +ggtitle('Observed Growth Rate Grouped by Year and Income Class')
```    

There does not appear to be any positive difference between the two. In fact, "high" label counties perform poorer than "low".

###Regression
We run a regression on the factors and investigate fit and coefficient on our high/low parameter.

We handle year by converting it to a numeric to capture general population growth as we move through time. What's most important is 
```{r, echo=F}
tr<-ma%>%
  separate(year,c('gar','yr'),"[[:punct:]]") %>%
  mutate(yr=as.numeric(yr), label.f=factor(label,levels=c("high","low"))) %>%
  select(-gar)

lm.obj<-lm(growth.rate~yr+label.f,data=tr)
```

How legit is this regression?
```{r}
#tr has same length as the residuals and fitted values
summary(lm.obj)
model.df<-data.frame(actual=tr$growth.rate,fitted.values=lm.obj$fitted.values,residuals=lm.obj$residuals,stringsAsFactors=F)

plot(lm.obj)
plot(lm.obj$residuals)
#can plot against a 0 line
ggplot(data=model.df,aes(x=fitted.values, y=residuals))+geom_point()
ggplot(data=model.df,aes(x=actual, y=residuals))+geom_point()
ggplot(data=model.df,aes(x=actual, y=fitted.values))+geom_point()
#plots of linear models
```
###Conclusion
The study was a good exercise in getting acquainted with BEA, census data as well as county shapefiles. We learned about the pitfalls aboue working with aggregated data like income per capita.     
**Improvements:**    
1) Conduct the study with a more narrowed scope such as region or state. Or conversely, create a hierarchy across the nation - giving due respect to mobility across states.         
2) Treat the earnings data differently or work on only one end of the spectrum of earnings.    
3) Introduce more variables.     
4) Conduct a longer study.    
5) Run a clustering algorithm versus our proprietary version
